{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6a95e03",
   "metadata": {},
   "source": [
    "# [Pytorch basic document]\n",
    "\n",
    "-----\n",
    "\n",
    "pytorch를 활용하여 만들 수 있는 model은 무궁무진하지만 기초를 알지 못한다면 원하는 모델을 만들기는 쉽지 않다\n",
    "\n",
    "해당 project에서는 pytorch의 기능과 함수들에 대해서 자세하게 보고 활용하여 활용 방법에 대해서 다루는 것을 목적으로 한다.\n",
    "\n",
    "- Python Version : Python 3.13.7\n",
    "\n",
    "참고 자료 : https://www.youtube.com/watch?v=kY14KfZQ1TI&list=PLCC34OHNcOtpcgR9LEYSdi9r7XIbpkpK1\n",
    "\n",
    "참고자료 : https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "-----\n",
    "\n",
    "## 목차(contents)\n",
    "\n",
    "- 1. Tensor (Tensor 생성, Tensor의 Attribution)\n",
    "- 2. Tensor Operations\n",
    "- 3. Tensor Math\n",
    "- 4. Neural Network (make, train, evaluate(using test, new data), save & load model)\n",
    "- 5. CNN(Convolution Neural Network) - Image fliter, Image kernel, Convolutional Layer & RGB, Pooling Layer, Train, Test, Checking Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "19728f7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (25.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.9.1)\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->torch) (3.0.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (2.3.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pytorch install, numpy\n",
    "\n",
    "%pip install --upgrade pip\n",
    "%pip install torch\n",
    "%pip install --upgrade torch\n",
    "%pip install numpy\n",
    "%pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed338f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch 모델 import\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6029692d",
   "metadata": {},
   "source": [
    "# 1. Tensor란 무엇인가?\n",
    "---\n",
    "\n",
    "tensor(텐서)는 array나 matrix와 유사한 특수 자료구조로, pytorch에서는 tensor를 통해 model으로의 input과 output 뿐만 아닌 모델의 매개변수를 encoding하는 과정에 사용된다.\n",
    "\n",
    "tensor는 numpy의 ndarray와 유사하지만, GPU나 연산 가속을 위한 특수 하드웨어에서 실행 가능하다는 점이 다르다.\n",
    "\n",
    "---\n",
    "\n",
    "### Tensor의 종류\n",
    "\n",
    "|Dimension|tensor 종류|명칭|\n",
    "|:---:|:---:|:---:|\n",
    "|0 dimension tensor|zeroth-order-tenso|scalar|\n",
    "|1 dimension tensor|First-order-tensor|vector|\n",
    "|2 dimension tensor|Second-order-tensor|matrix|\n",
    "|3 dimension tensor|Third-order-tensor|tensor|\n",
    "|4 dimension tensor|Fourth-order-tensor|stack with 3 dimension tensor|\n",
    "|5 dimension tensor|Fifth-order-tensor|expand side with 4 dimension tensor|\n",
    "|...|...|...|\n",
    "\n",
    "![tensor의 종류](./tensor.png)\n",
    "\n",
    "참고자료 : https://tutorials.pytorch.kr/beginner/blitz/tensor_tutorial.html\n",
    "\n",
    "참고자료 : https://www.youtube.com/watch?v=2yBEZzQu8dA&list=PLCC34OHNcOtpcgR9LEYSdi9r7XIbpkpK1&index=2\n",
    "\n",
    "이미지 출처 : Pytorch로 시작하는 딥러닝 입문\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840bdbe5",
   "metadata": {},
   "source": [
    "## 1) Tensor 초기화 및 생성\n",
    "---\n",
    "\n",
    "tensor 생성 당시 자료형은 자동으로 유추하여 맞게 생성되기 때문에 따로 지정할 필요는 없다.\n",
    "\n",
    "크게 tensor의 생성 방법으로는 4가지로 나뉜다.\n",
    "\n",
    "1) data -> tensor\n",
    "2) data -> numpy array -> tensor\n",
    "3) original_tensor -> duplicate_tensor(attribute 유지 O)\n",
    "4) original_tensor -> duplicate_tensor(attribute 유지 X)\n",
    "5) data(zero, one, random) -> tensor\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "221a848d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 생성 (data -> tensor)\n",
    "\n",
    "data = [[1, 2],[3, 4]] # 데이터 배열 생성\n",
    "x_data = torch.tensor(data) # data를 tensor로 변환\n",
    "\n",
    "print(x_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c2c9b08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2],\n",
      "        [3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 생성 (data -> numpy array -> tensor)\n",
    "import numpy as np\n",
    "\n",
    "np_array = np.array(data) # numpy array 생성\n",
    "x_np = torch.from_numpy(np_array) # numpy array -> tensor로 변환\n",
    "\n",
    "print(x_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60393171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ones Tensor : \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor : \n",
      " tensor([[0.8947, 0.5516],\n",
      "        [0.7662, 0.3233]], dtype=torch.float64) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensor 생성 (다른 tensor -> 새로운 tensor)\n",
    "# 속성의 유지여부에 따라 ones_like, rand_like를 나눠서 쓰는데 속성에는 tensor의 shape과 datatype이 해당된다.\n",
    "\n",
    "tensor_ones = torch.ones_like(x_data) # 기존 tensor의 속성을 유지\n",
    "tensor_rand = torch.rand_like(x_data, dtype=float) # 기존 tensor의 속성을 변경\n",
    "\n",
    "print(f\"Ones Tensor : \\n {tensor_ones} \\n\")\n",
    "print(f\"Random Tensor : \\n {tensor_rand} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "cf057049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand_tensor \n",
      " tensor([[0.2095, 0.3311, 0.6936, 0.6714, 0.6296],\n",
      "        [0.3809, 0.3754, 0.3260, 0.3126, 0.8773],\n",
      "        [0.1281, 0.8869, 0.2443, 0.2409, 0.5985]]) \n",
      "\n",
      "ones_tensor \n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]]) \n",
      "\n",
      "zeros_tensor \n",
      " tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensor 생성을 random(랜덤값) 또는 constant(상수값)을 활용하여 생성하는 방법\n",
    "\n",
    "shape = (3, 5) #tensor의 차원을 column:3, row:5로 setting\n",
    "\n",
    "rand_tensor = torch.rand(shape) #랜덤값으로 shape에 맞게 tensor를 생성\n",
    "ones_tensor = torch.ones(shape) #모든 원소값은 1로 shape에 맞게 tensor 생성\n",
    "zeros_tensor = torch.zeros(shape) #모든 원소값을 0으로 shape에 맞게 tensor 생성\n",
    "\n",
    "# 각 tensor의 구조 및 원소의 상태를 print하여 확인\n",
    "print(f\"rand_tensor \\n {rand_tensor} \\n\")\n",
    "print(f\"ones_tensor \\n {ones_tensor} \\n\")\n",
    "print(f\"zeros_tensor \\n {zeros_tensor} \\n\")\n",
    "\n",
    "# numpy array를 기준으로 원소 값 초기화를 통해 tensor 생성도 가능함\n",
    "\n",
    "# torch.ones_like(t) # t와 같은 사이즈의 1로 채워진 tensor를 출력\n",
    "# torch.zeros_like(t) # 모든 값을 0으로 \n",
    "# torch.ones(size=[]) # 1로 채워진 size 차원의 tensor를 출력\n",
    "# torch.zeros(size=[]) # 0으로 채워진 size 차원의 tensor를 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd64175",
   "metadata": {},
   "source": [
    "## 2) Tensor의 Attribute(속성)\n",
    "\n",
    "---\n",
    "\n",
    "tensor의 attribute(속성)은 tensor의 shape, datatype 및 store device location을 의미한다.\n",
    "\n",
    "따라서, 각 tensor는 이런 attribute를 모두 가지고 있기 때문에 특정 tensor의 shape/dtype/device의 속성을 확인이 가능하다\n",
    "\n",
    "추가적인 속성 확인인 size(), dim()의 경우에는 메소드(함수)이기 때문에 (괄호)를 뺴먹지 않도록 주의해야 함.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a737136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_sample \n",
      " tensor([[0.5339, 0.0088, 0.1896],\n",
      "        [0.4668, 0.3965, 0.6455]]) \n",
      "\n",
      "Shape of tensor : torch.Size([2, 3])\n",
      "Size of tensor : torch.Size([2, 3])\n",
      "Dimension of tensor : 2\n",
      "Datatype of tensor : torch.float32\n",
      "Device tensor is stored on : cpu\n"
     ]
    }
   ],
   "source": [
    "# tensor 랜덤값으로 생성\n",
    "\n",
    "tensor_sample = torch.rand(2, 3) #랜덤 값을 활용하여 생성한 2x3 tensor\n",
    "\n",
    "print(f\"tensor_sample \\n {tensor_sample} \\n\") #tensor_sample을 출력\n",
    "print(f\"Shape of tensor : {tensor_sample.shape}\") #tensor_sample의 shape을 출력\n",
    "print(f\"Size of tensor : {tensor_sample.size()}\") #tensor_sample의 size를 출력\n",
    "print(f\"Dimension of tensor : {tensor_sample.dim()}\") #tensor_sample의 dimension을 출력\n",
    "print(f\"Datatype of tensor : {tensor_sample.dtype}\") #tensor_sample의 데이터 타입을 출력\n",
    "print(f\"Device tensor is stored on : {tensor_sample.device}\") #tensor_sample이 저장된 device의 location을 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101bd023",
   "metadata": {},
   "source": [
    "# 2. Tensor Operation(텐서 연산)\n",
    "\n",
    "---\n",
    "\n",
    "선형대수 연산 기능, 수학적 계산, 인덱싱, 슬라이싱, 임의 샘플링 등 tensor 구조로 연산 기능을 지원하는 pytorch에는 다양한 함수가 존재한다.\n",
    "\n",
    "가장 기본적인 Reshape, Slice 기법을 확인 후 수학적 tensor 연산인 Add, Subtract, Multiply, Divide, Remainders, Exponent, Shorthand, Longhand, Reassignment에 대해서 예시 코드를 통해 보도록 하겠다.\n",
    "\n",
    "추가적으로 필요하다고 생각되는 tensor operation에 대해서는 하단의 내용을 추가할 예정이다.\n",
    "\n",
    "Pytorch의 Tensor 연산 document : https://docs.pytorch.org/docs/stable/torch.html\n",
    "\n",
    "참고자료 : https://www.youtube.com/watch?v=Ta3z9vZaoMc&list=PLCC34OHNcOtpcgR9LEYSdi9r7XIbpkpK1&index=4\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbe2826",
   "metadata": {},
   "source": [
    "### 0) tensor 생성 함수 정리\n",
    "\n",
    "| 자료형(category) | dtype | 설명 | 대표 생성 함수 | dtype 지정 예시 |\n",
    "|------------------|--------|--------|---------------------|-----------------------|\n",
    "| **부호 없는 정수** | `torch.uint8` | Unsigned 8-bit | `torch.UInt8Tensor()` | `dtype=torch.uint8` |\n",
    "| **정수** | `torch.int8` | Signed 8-bit | `torch.Int8Tensor()` | `dtype=torch.int8` |\n",
    "| | `torch.int16` (`torch.short`) | 16-bit | `torch.ShortTensor()` | `dtype=torch.int16` |\n",
    "| | `torch.int32` (`torch.int`) | 32-bit | `torch.IntTensor()` | `dtype=torch.int32` |\n",
    "| | `torch.int64` (`torch.long`) | 64-bit (가장 많이 사용) | `torch.LongTensor()` | `dtype=torch.int64` |\n",
    "| **부동소수점(Float)** | `torch.float16` (`torch.half`) | Half precision (16-bit) | `torch.HalfTensor()` | `dtype=torch.float16` |\n",
    "| | `torch.float32` (`torch.float`) | Single precision (32-bit) | `torch.FloatTensor()` | `dtype=torch.float32` |\n",
    "| | `torch.float64` (`torch.double`) | Double precision (64-bit) | `torch.DoubleTensor()` | `dtype=torch.float64` |\n",
    "| **Boolean** | `torch.bool` | True/False | `torch.BoolTensor()` | `dtype=torch.bool` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "84dc261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor 생성\n",
    "\n",
    "tensor_a = torch.tensor([1, 2, 3, 4])\n",
    "tensor_b = torch.tensor([5, 6, 7, 8])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf45366",
   "metadata": {},
   "source": [
    "### 1) Tensor addition (덧셈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7716de0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  8, 10, 12])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor addition\n",
    "\n",
    "tensor_a + tensor_b\n",
    "\n",
    "# another way to write addition\n",
    "\n",
    "tensor_a.add(tensor_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f450667",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 6,  8, 10, 12])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor addition longhead(일반적) version\n",
    "\n",
    "torch.add(tensor_a, tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22383a",
   "metadata": {},
   "source": [
    "### 2) tensor subtraction (뻴셈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5315156d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtraction\n",
    "\n",
    "tensor_b - tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "de37786d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4, 4, 4, 4])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# subtraction function\n",
    "\n",
    "torch.sub(tensor_b, tensor_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632e77fa",
   "metadata": {},
   "source": [
    "### 3) tensor multiplication (곱셈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "297fa9d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 12, 21, 32])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplication\n",
    "\n",
    "tensor_a * tensor_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "90f9e99f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 12, 21, 32])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# multiplication longhand\n",
    "\n",
    "torch.mul(tensor_a, tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc70a88",
   "metadata": {},
   "source": [
    "### 4) tensor division (나눗셈)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94cba3ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5.0000, 3.0000, 2.3333, 2.0000])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# division\n",
    "\n",
    "tensor_b / tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b44fb408",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.3333, 0.4286, 0.5000])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# division longhand\n",
    "\n",
    "torch.div(tensor_a, tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774e3a5",
   "metadata": {},
   "source": [
    "### 5) tensor reminder modulus (나머지 연산)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a117470f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reminder modulus(나머지)\n",
    "\n",
    "tensor_b % tensor_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc4db785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 1, 0])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remainder longhand (나머지)\n",
    "\n",
    "torch.remainder(tensor_b, tensor_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8579d86",
   "metadata": {},
   "source": [
    "### 6) tensor exponents/power (지수와 제곱근) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "990aee48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,    64,  2187, 65536])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exponents / power (제곱)\n",
    "\n",
    "torch.pow(tensor_a, tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974ca0f",
   "metadata": {},
   "source": [
    "### 7) reassignment (기존 tensor 값 재할당)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8240e974",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([26, 32, 38, 44])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reassignment\n",
    "\n",
    "tensor_a + tensor_b\n",
    "\n",
    "tensor_a = tensor_a + tensor_b\n",
    "\n",
    "tensor_a.add_(tensor_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924bd262",
   "metadata": {},
   "source": [
    "### 8) 추가적인 tensor 속성들 (내적 곱셈, 원소별 곱셈, 형태 변환, cat, stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "353074e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 생성\n",
    "data_1 = [[1, 3, 4], [5, 6, 7], [9, 10, 11]]\n",
    "data_2 = [[11, 13, 14], [15, 16, 17], [19, 110, 111]]\n",
    "\n",
    "# numpy array로 변환\n",
    "t1 = np.array(data_1)\n",
    "t2 = np.array(data_2)\n",
    "\n",
    "# numpy array -> tensor로 변환\n",
    "tensor_1 = torch.FloatTensor(t1)\n",
    "tensor_2 = torch.FloatTensor(t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43b82cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(36.2222)\n",
      "tensor(326.)\n",
      "tensor(111.)\n",
      "tensor(8)\n"
     ]
    }
   ],
   "source": [
    "# tensor의 세부 속성 (mean, sum, max, argmax)\n",
    "\n",
    "print(tensor_2.mean()) #tensor 원소의 전체값 평균\n",
    "print(tensor_2.sum()) #tensor 원소의 전체값의 합 (dim=0)을 달면 차원제거 가능\n",
    "print(tensor_2.max()) #tensor 원소 중 가장 큰 값 (dim=0)을 달면 차원제거 가능\n",
    "print(tensor_2.argmax()) #tensor의 모든 원소 중 가장 큰 값의 index 출력 (dim=0)을 탈면 차원제거 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "47ff9868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix Multiplication result : \n",
      "tensor([[ 132.,  501.,  509.],\n",
      "        [ 278.,  931.,  949.],\n",
      "        [ 458., 1487., 1517.]])\n",
      "\n",
      "element-wise Multiplication : \n",
      "tensor([[  11.,   39.,   56.],\n",
      "        [  75.,   96.,  119.],\n",
      "        [ 171., 1100., 1221.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# tensor의 내적 곱셈 및 원소별 곱셈\n",
    "\n",
    "result1 = tensor_1.matmul(tensor_2)\n",
    "result2 = tensor_1.mul(tensor_2)\n",
    "\n",
    "print(f\"Matrix Multiplication result : \\n{result1}\\n\")\n",
    "print(f\"element-wise Multiplication : \\n{result2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0f8135d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.],\n",
      "        [ 9., 10., 11.]])\n",
      "tensor([[ 1.,  3.,  4.],\n",
      "        [ 5.,  6.,  7.],\n",
      "        [ 9., 10., 11.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 1.,  3.,  4.]],\n",
       "\n",
       "        [[ 5.,  6.,  7.]],\n",
       "\n",
       "        [[ 9., 10., 11.]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor의 형태 변형 및 변환 (view(), squeeze(), unsqueeze())\n",
    "\n",
    "reshape_tensor = tensor_1.view([3, 3]) #tensor의 형태 변경(numpy의 reshape 기능)\n",
    "print(reshape_tensor)\n",
    "\n",
    "reshape_tensor.squeeze(1) #tensor의 차원을 제거\n",
    "print(reshape_tensor)\n",
    "\n",
    "reshape_tensor.unsqueeze(dim=1) #tensor의 차원을 특정 위치에 차원을 추가함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1464fdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1.,   3.,   4.],\n",
      "        [  5.,   6.,   7.],\n",
      "        [  9.,  10.,  11.],\n",
      "        [ 11.,  13.,  14.],\n",
      "        [ 15.,  16.,  17.],\n",
      "        [ 19., 110., 111.]])\n",
      "tensor([[[  1.,   3.,   4.],\n",
      "         [  5.,   6.,   7.],\n",
      "         [  9.,  10.,  11.]],\n",
      "\n",
      "        [[ 11.,  13.,  14.],\n",
      "         [ 15.,  16.,  17.],\n",
      "         [ 19., 110., 111.]]])\n"
     ]
    }
   ],
   "source": [
    "# tensor 병합(cat()), 연결(stack())\n",
    "\n",
    "new_tensor = torch.cat([tensor_1, tensor_2], dim=0) #tensor 병합하여 하나의 tensor 생성\n",
    "new_stack_tensor = torch.stack([tensor_1, tensor_2], dim=0) #tensor를 연결 - 병합하지 않고 연결만 진행함\n",
    "\n",
    "print(new_tensor)\n",
    "print(new_stack_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd783aa",
   "metadata": {},
   "source": [
    "# 3. Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad5fef7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
